{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65989779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Retriever Implementation\n",
    "def retrieve_chunks(question, model, vector_store, k=5):\n",
    "    # Step 1: Embed the question\n",
    "    question_embedding = model.embed(question)  # Use the embedding model from Task 2\n",
    "\n",
    "    # Step 2: Perform similarity search\n",
    "    distances, indices = vector_store.search(question_embedding, k)\n",
    "    \n",
    "    # Retrieve the top-k relevant chunks\n",
    "    retrieved_chunks = [vector_store.documents[i] for i in indices]\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Step 2: Prompt Engineering\n",
    "def create_prompt(context, question):\n",
    "    prompt_template = (\n",
    "        \"You are a financial analyst assistant for CrediTrust. \"\n",
    "        \"Your task is to answer questions about customer complaints. \"\n",
    "        \"Use the following retrieved complaint excerpts to formulate your answer. \"\n",
    "        \"If the context doesn't contain the answer, state that you don't have enough information.\\n\"\n",
    "        f\"Context: {context}\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    return prompt_template\n",
    "\n",
    "# Step 3: Generator Implementation\n",
    "def generate_answer(prompt):\n",
    "    generator = pipeline('text-generation', model='gpt-3.5-turbo')  # Adjust model as needed\n",
    "    response = generator(prompt, max_length=150)[0]['generated_text']\n",
    "    return response.strip()\n",
    "\n",
    "# Step 4: RAG Pipeline\n",
    "def rag_pipeline(question, model, vector_store):\n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_chunks = retrieve_chunks(question, model, vector_store)\n",
    "    \n",
    "    # Combine the retrieved chunks into a single context string\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(context, question)\n",
    "    \n",
    "    # Generate the answer\n",
    "    answer = generate_answer(prompt)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example of Qualitative Evaluation\n",
    "evaluation_data = [\n",
    "    {\n",
    "        \"Question\": \"What are the common issues customers face?\",\n",
    "        \"Generated Answer\": \"\",\n",
    "        \"Retrieved Sources\": [],\n",
    "        \"Quality Score\": None,\n",
    "        \"Comments/Analysis\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Question\": \"How does CrediTrust handle complaints?\",\n",
    "        \"Generated Answer\": \"\",\n",
    "        \"Retrieved Sources\": [],\n",
    "        \"Quality Score\": None,\n",
    "        \"Comments/Analysis\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"Question\": \"What feedback do customers give about services?\",\n",
    "        \"Generated Answer\": \"\",\n",
    "        \"Retrieved Sources\": [],\n",
    "        \"Quality Score\": None,\n",
    "        \"Comments/Analysis\": \"\"\n",
    "    },\n",
    "    # Add more questions as needed\n",
    "]\n",
    "\n",
    "# Evaluate RAG pipeline\n",
    "for entry in evaluation_data:\n",
    "    question = entry[\"Question\"]\n",
    "    generated_answer = rag_pipeline(question, model, vector_store)\n",
    "    entry[\"Generated Answer\"] = generated_answer\n",
    "    entry[\"Retrieved Sources\"] = retrieve_chunks(question, model, vector_store, k=2)  # Show 1-2 sources\n",
    "    entry[\"Quality Score\"] = int(input(f\"Rate the quality of the answer for '{question}' (1-5): \"))\n",
    "    entry[\"Comments/Analysis\"] = input(f\"Comments for '{question}': \")\n",
    "\n",
    "# Print evaluation results\n",
    "for entry in evaluation_data:\n",
    "    print(f\"Question: {entry['Question']}\")\n",
    "    print(f\"Generated Answer: {entry['Generated Answer']}\")\n",
    "    print(f\"Retrieved Sources: {entry['Retrieved Sources']}\")\n",
    "    print(f\"Quality Score: {entry['Quality Score']}\")\n",
    "    print(f\"Comments/Analysis: {entry['Comments/Analysis']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f85e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
